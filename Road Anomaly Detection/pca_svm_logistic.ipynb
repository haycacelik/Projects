{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKSimtgoYHu4"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6TyI43dYKO5",
        "outputId": "b3c5ba09-d76a-4466-f55b-7efa3773242b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-04T07:27:24.404716Z",
          "iopub.status.busy": "2024-01-04T07:27:24.404243Z",
          "iopub.status.idle": "2024-01-04T07:27:24.420848Z",
          "shell.execute_reply": "2024-01-04T07:27:24.419627Z",
          "shell.execute_reply.started": "2024-01-04T07:27:24.404669Z"
        },
        "id": "sy3BY-AIYHu7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.applications import vgg16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from PIL import Image\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-04T07:27:28.799919Z",
          "iopub.status.busy": "2024-01-04T07:27:28.799425Z",
          "iopub.status.idle": "2024-01-04T07:27:48.945428Z",
          "shell.execute_reply": "2024-01-04T07:27:48.943717Z",
          "shell.execute_reply.started": "2024-01-04T07:27:28.799826Z"
        },
        "id": "kp-ASByMYHu8",
        "outputId": "877e4ba4-b1e6-4ea4-9468-8d922d9b6030",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/data/split_data/train/normal/226.jpg\n"
          ]
        }
      ],
      "source": [
        "# Load images\n",
        "def load_im():\n",
        "    input_im, input_label, data_split = [], [], []\n",
        "    resize = (224, 224)\n",
        "    # Loop in folders\n",
        "    for dirname, _, filenames in os.walk('/content/drive/MyDrive/data/split_data'):\n",
        "        for filename in filenames:\n",
        "            photo_path = os.path.join(dirname, filename)\n",
        "            photo_class = dirname.split('/')[-1]\n",
        "            try:\n",
        "                read_im = cv2.imread(photo_path)\n",
        "                input_im.append(cv2.resize(read_im, resize))\n",
        "                # potholes == 1\n",
        "                if photo_class == 'pothole':\n",
        "                    input_label.append(1)\n",
        "                # speedbump == 2\n",
        "                elif photo_class == 'speedbump':\n",
        "                    input_label.append(2)\n",
        "                # normal == 0\n",
        "                elif photo_class == 'normal':\n",
        "                    input_label.append(0)\n",
        "\n",
        "                # Determine data split (you may need to adapt this based on your actual data structure)\n",
        "                if \"train\" in dirname:\n",
        "                    data_split.append('train')\n",
        "                elif \"test\" in dirname:\n",
        "                    data_split.append('test')\n",
        "                else:\n",
        "                    # If the folder does not specify train or test, you may need to handle it accordingly\n",
        "                    data_split.append('unknown_split')\n",
        "            except:\n",
        "                print(photo_path)\n",
        "    # return list of images, another list of corresponding labels, and the data split information\n",
        "    return input_im, input_label, data_split\n",
        "\n",
        "input_im, input_label, data_split = load_im()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-04T07:28:26.643198Z",
          "iopub.status.busy": "2024-01-04T07:28:26.642752Z",
          "iopub.status.idle": "2024-01-04T07:28:26.788800Z",
          "shell.execute_reply": "2024-01-04T07:28:26.787776Z",
          "shell.execute_reply.started": "2024-01-04T07:28:26.643140Z"
        },
        "id": "EdT9XGSSYHu-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_test_split(input_im, input_label, data_split):\n",
        "    # Identify indices for train and test sets\n",
        "    train_indices = [i for i, split in enumerate(data_split) if split == 'train']\n",
        "    test_indices = [i for i, split in enumerate(data_split) if split == 'test']\n",
        "\n",
        "    # Split the data\n",
        "    train_x, test_x = np.take(input_im, train_indices, axis=0), np.take(input_im, test_indices, axis=0)\n",
        "    train_y, test_y = np.take(input_label, train_indices, axis=0), np.take(input_label, test_indices, axis=0)\n",
        "\n",
        "    # Return train and test sets for both images and labels\n",
        "    return train_x, test_x, train_y, test_y\n",
        "\n",
        "# Use existing splits\n",
        "train_x, test_x, train_y, test_y = train_test_split(input_im, input_label, data_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-04T07:28:31.422069Z",
          "iopub.status.busy": "2024-01-04T07:28:31.421629Z",
          "iopub.status.idle": "2024-01-04T07:28:31.431743Z",
          "shell.execute_reply": "2024-01-04T07:28:31.430401Z",
          "shell.execute_reply.started": "2024-01-04T07:28:31.422019Z"
        },
        "id": "BBU736PLYHu-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def append_im(input_im, input_label, im_iterator):\n",
        "    input_label_n = input_label.copy()\n",
        "    input_im_n = input_im.copy()\n",
        "    for i in range(len(im_iterator)):\n",
        "        im = im_iterator[i]\n",
        "        im = im.astype('uint8')\n",
        "        im_lbl = [input_label[i]]\n",
        "        input_im_n = np.append(input_im_n, im, axis = 0)\n",
        "        input_label_n = np.append(input_label_n, im_lbl, axis = 0)\n",
        "    return input_im_n, input_label_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhUR0-1eYHu-"
      },
      "source": [
        " ### PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-04T07:28:35.865430Z",
          "iopub.status.busy": "2024-01-04T07:28:35.865002Z",
          "iopub.status.idle": "2024-01-04T07:29:36.661356Z",
          "shell.execute_reply": "2024-01-04T07:29:36.660023Z",
          "shell.execute_reply.started": "2024-01-04T07:28:35.865376Z"
        },
        "id": "x68ZxKuTYHu_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Flipping\n",
        "flip_data_generator = ImageDataGenerator(horizontal_flip = True)\n",
        "im_iterator = flip_data_generator.flow(train_x, batch_size = 1, shuffle = False)\n",
        "input_im_n, input_label_n = append_im(train_x, train_y, im_iterator)\n",
        "\n",
        "# Reshape\n",
        "nx, ny, nz = train_x.shape[1], train_x.shape[2], train_x.shape[3]\n",
        "train_x_nn, test_x_nn = input_im_n, test_x\n",
        "train_x = input_im_n.reshape((input_im_n.shape[0], nx * ny * nz)) / 255\n",
        "test_x = test_x.reshape((test_x.shape[0], nx * ny * nz)) / 255\n",
        "train_y = input_label_n.reshape((input_label_n.shape[0], 1))\n",
        "test_y = test_y.reshape((test_y.shape[0], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-04T07:29:43.680855Z",
          "iopub.status.busy": "2024-01-04T07:29:43.680044Z",
          "iopub.status.idle": "2024-01-04T07:31:54.982206Z",
          "shell.execute_reply": "2024-01-04T07:31:54.981034Z",
          "shell.execute_reply.started": "2024-01-04T07:29:43.680777Z"
        },
        "id": "umRaVl0tYHu_",
        "outputId": "1344ea6a-5155-477e-c685-bf410fcbbdd4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.24292623 0.37603674 0.42919601 ... 1.         1.         1.        ]\n"
          ]
        }
      ],
      "source": [
        "# Dimensionality reduction - PCA\n",
        "im_pca = PCA()\n",
        "im_pca.fit(train_x)\n",
        "variance_explained_list = im_pca.explained_variance_ratio_.cumsum()\n",
        "print(variance_explained_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-04T07:31:59.823741Z",
          "iopub.status.busy": "2024-01-04T07:31:59.823308Z",
          "iopub.status.idle": "2024-01-04T07:32:14.521395Z",
          "shell.execute_reply": "2024-01-04T07:32:14.519178Z",
          "shell.execute_reply.started": "2024-01-04T07:31:59.823685Z"
        },
        "id": "N_5DCZ2kYHu_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_x_pca = im_pca.transform(test_x)\n",
        "train_x_pca = im_pca.transform(train_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-04T07:32:19.345068Z",
          "iopub.status.busy": "2024-01-04T07:32:19.344592Z",
          "iopub.status.idle": "2024-01-04T07:37:41.964138Z",
          "shell.execute_reply": "2024-01-04T07:37:41.962846Z",
          "shell.execute_reply.started": "2024-01-04T07:32:19.345012Z"
        },
        "id": "SkD9N8ZFYHvA",
        "outputId": "5167692f-954e-470e-b38c-403f0db45ec0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regularization parameters:  0.1 Accuracy 0.7670807453416149\n",
            "Regularization parameters:  0.2 Accuracy 0.8198757763975155\n",
            "Regularization parameters:  0.30000000000000004 Accuracy 0.8490683229813665\n",
            "Regularization parameters:  0.4 Accuracy 0.8801242236024844\n",
            "Regularization parameters:  0.5 Accuracy 0.9024844720496894\n",
            "Regularization parameters:  0.6000000000000001 Accuracy 0.9180124223602485\n",
            "Regularization parameters:  0.7000000000000001 Accuracy 0.9322981366459627\n",
            "Regularization parameters:  0.8 Accuracy 0.94472049689441\n",
            "Regularization parameters:  0.9 Accuracy 0.9577639751552796\n",
            "Regularization parameters:  1.0 Accuracy 0.9645962732919254\n",
            "Regularization parameters:  1.1 Accuracy 0.9720496894409938\n",
            "Regularization parameters:  1.2000000000000002 Accuracy 0.9770186335403727\n",
            "Regularization parameters:  1.3 Accuracy 0.9838509316770186\n",
            "Regularization parameters:  1.4000000000000001 Accuracy 0.9850931677018634\n",
            "Regularization parameters:  1.5 Accuracy 0.9881987577639751\n",
            "Regularization parameters:  1.6 Accuracy 0.9888198757763975\n",
            "Regularization parameters:  1.7000000000000002 Accuracy 0.9894409937888199\n",
            "Regularization parameters:  1.8 Accuracy 0.9906832298136646\n",
            "Regularization parameters:  1.9000000000000001 Accuracy 0.9937888198757764\n",
            "Regularization parameters:  2.0 Accuracy 0.9944099378881988\n",
            "Regularization parameters:  2.1 Accuracy 0.9950310559006211\n",
            "Regularization parameters:  2.2 Accuracy 0.9956521739130435\n",
            "Regularization parameters:  2.3000000000000003 Accuracy 0.9975155279503105\n",
            "Regularization parameters:  2.4000000000000004 Accuracy 0.9975155279503105\n",
            "Regularization parameters:  2.5 Accuracy 0.9975155279503105\n",
            "Regularization parameters:  2.6 Accuracy 0.9975155279503105\n",
            "Regularization parameters:  2.7 Accuracy 0.9975155279503105\n",
            "Regularization parameters:  2.8000000000000003 Accuracy 0.9975155279503105\n",
            "Regularization parameters:  2.9000000000000004 Accuracy 0.9975155279503105\n"
          ]
        }
      ],
      "source": [
        "# SVM\n",
        "def svm_grid_search(C, kernel, train_x, train_y):\n",
        "    accuracy_score_list = []\n",
        "\n",
        "    for c in C:\n",
        "        # Model training\n",
        "        svmClassifier = svm.SVC(C = c, kernel = kernel)\n",
        "        svmClassifier.fit(train_x, train_y.ravel())\n",
        "        # Prediction on test set\n",
        "        pred_y = svmClassifier.predict(train_x)\n",
        "        # Accuracy\n",
        "        accuracy = accuracy_score(train_y, pred_y)\n",
        "        accuracy_score_list.append(accuracy)\n",
        "        print('Regularization parameters: ', c, 'Accuracy', accuracy)\n",
        "\n",
        "    max_accurarcy_id = accuracy_score_list.index(max(accuracy_score_list))\n",
        "    return C[max_accurarcy_id]\n",
        "\n",
        "C, kernel = [0.1 * i for i in range(1, 30)], 'rbf'\n",
        "opt_C = svm_grid_search(C, kernel, train_x_pca, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-04T07:37:54.954763Z",
          "iopub.status.busy": "2024-01-04T07:37:54.954024Z",
          "iopub.status.idle": "2024-01-04T07:38:01.637587Z",
          "shell.execute_reply": "2024-01-04T07:38:01.636366Z",
          "shell.execute_reply.started": "2024-01-04T07:37:54.954658Z"
        },
        "id": "CoRKvR60YHvA",
        "outputId": "cbd56c0b-cc03-4d85-d731-9e829dfa27f7",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7326732673267327\n"
          ]
        }
      ],
      "source": [
        "# Test set\n",
        "svmClassifier = svm.SVC(C = opt_C, kernel = kernel)\n",
        "svmClassifier.fit(train_x_pca, train_y.ravel())\n",
        "pred_y = svmClassifier.predict(test_x_pca)\n",
        "accuracy = accuracy_score(test_y, pred_y)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-04T07:38:47.855927Z",
          "iopub.status.busy": "2024-01-04T07:38:47.855464Z",
          "iopub.status.idle": "2024-01-04T07:38:47.883096Z",
          "shell.execute_reply": "2024-01-04T07:38:47.882063Z",
          "shell.execute_reply.started": "2024-01-04T07:38:47.855847Z"
        },
        "id": "PP6N9fplYHvB",
        "outputId": "d50f3aef-e42f-4708-d0d1-ede74cf49b11",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.16470588 0.21960784 0.14901961 ... 0.57254902 0.57254902 0.51764706]\n",
            " [0.69411765 0.7372549  0.75294118 ... 0.19607843 0.17254902 0.16862745]\n",
            " [0.4745098  0.48627451 0.50196078 ... 0.3372549  0.31372549 0.31764706]\n",
            " ...\n",
            " [0.70588235 0.48235294 0.30588235 ... 0.74117647 0.7372549  0.77254902]\n",
            " [0.29411765 0.52941176 0.69803922 ... 0.3254902  0.30588235 0.28627451]\n",
            " [0.64705882 0.37647059 0.21960784 ... 0.59215686 0.65882353 0.81176471]]\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - 2s 12ms/step - loss: 24.7956 - accuracy: 0.4534\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 3.4446 - accuracy: 0.6398\n",
            "Epoch 3/10\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 2.6244 - accuracy: 0.6602\n",
            "Epoch 4/10\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 2.8345 - accuracy: 0.6472\n",
            "Epoch 5/10\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 2.1135 - accuracy: 0.6919\n",
            "Epoch 6/10\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 1.4885 - accuracy: 0.7503\n",
            "Epoch 7/10\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 3.3890 - accuracy: 0.6770\n",
            "Epoch 8/10\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 2.5682 - accuracy: 0.7099\n",
            "Epoch 9/10\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 1.8817 - accuracy: 0.7466\n",
            "Epoch 10/10\n",
            "51/51 [==============================] - 1s 15ms/step - loss: 1.9496 - accuracy: 0.7466\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f7848bd12d0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Logistic Regression\n",
        "def Logistic():\n",
        "    logistic_model = Sequential()\n",
        "    logistic_model.add(Dense(3, activation=\"softmax\"))\n",
        "    return logistic_model\n",
        "\n",
        "# Compile Model\n",
        "logistic_model = Logistic()\n",
        "# Training Model\n",
        "logistic_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "# Training Model\n",
        "print(train_x)\n",
        "print(train_y)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Create the encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "# Fit and transform the data\n",
        "one_hot = encoder.fit_transform(train_y)\n",
        "\n",
        "# do one hot encoding to lables\n",
        "logistic_model.fit(train_x, one_hot, batch_size = 32, epochs = 10, verbose = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-04T07:39:11.865727Z",
          "iopub.status.busy": "2024-01-04T07:39:11.865268Z",
          "iopub.status.idle": "2024-01-04T07:39:11.994639Z",
          "shell.execute_reply": "2024-01-04T07:39:11.993487Z",
          "shell.execute_reply.started": "2024-01-04T07:39:11.865655Z"
        },
        "id": "RgrIBPs1YHvB",
        "outputId": "12eb5aa4-6f73-4abc-b326-0c0f7e7e89b2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['loss', 'accuracy']\n",
            "[5.4310479164123535, 0.5247524976730347]\n"
          ]
        }
      ],
      "source": [
        "# Test set\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Create the encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "# Fit and transform the data\n",
        "one_hot = encoder.fit_transform(test_y)\n",
        "\n",
        "print(logistic_model.metrics_names)\n",
        "print(logistic_model.evaluate(test_x, one_hot, verbose = 0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4258230,
          "sourceId": 7335094,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 29907,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
